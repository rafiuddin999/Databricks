{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import when\ndata = [(1,'maheer','M',2000,'dev'),(2,'srjan','M',3000,'IT'),(3,'maheer','F',4000,'HR'),(3,'maheer','F',4000,'HR'),(4,'kalyan','M',4000,'HR')]\nschema = ['id','name','gender','salary','dep']\ndf=spark.createDataFrame(data,schema)\ndf.show()\n#df1 = df.select(df.id,\\\n#                 df.name,\\\n#                 when(df.gender=='M','male')\\\n#                 .when(df.gender=='F','female')\\\n#                 .otherwise('unknown')\\\n#                 .alias('gender'))\n# df1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"40142384-c385-484c-85d6-253761c94162","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+------+------+---+\n| id|  name|gender|salary|dep|\n+---+------+------+------+---+\n|  1|maheer|     M|  2000|dev|\n|  2| srjan|     M|  3000| IT|\n|  3|maheer|     F|  4000| HR|\n|  3|maheer|     F|  4000| HR|\n|  4|kalyan|     M|  4000| HR|\n+---+------+------+------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["rows = [(1,'maheer','300'),(2,'rafi','200')]\ncolumn = ['id','name','salary']\n\ndafa = spark.createDataFrame(rows,column)\ndafa.show()\ndafa.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6530dcba-2e90-4a89-9724-92b5052afb20","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|maheer|   300|\n|  2|  rafi|   200|\n+---+------+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndafa=dafa.withColumn(colName='salary',col=col('salary').cast('Integer')) # here the datatype changed from the string to integer using withColumn\n\n\ndafa.withColumn(\"CopiedColumn\",col(\"salary\")* -1).show()\n# +---+------+------+------------+\n# | id|  name|salary|CopiedColumn|\n# +---+------+------+------------+\n# |  1|maheer|   300|        -300|\n# |  2|  rafi|   200|        -200|\n# +---+------+------+------------+\n\ndafa.withColumnRenamed('salary','salary_amount').show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"06db46ea-ab24-4906-b222-5f60e1396714","inputWidgets":{},"title":"withColumn & withColumnRenamed"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+------+------------+\n| id|  name|salary|CopiedColumn|\n+---+------+------+------------+\n|  1|maheer|   300|        -300|\n|  2|  rafi|   200|        -200|\n+---+------+------+------------+\n\n+---+------+-------------+\n| id|  name|salary_amount|\n+---+------+-------------+\n|  1|maheer|          300|\n|  2|  rafi|          200|\n+---+------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StringType,StructField,StructType,IntegerType\nrow = [(1,'maheer',300),(2,'rafi',200)]\n\nschem = StructType([\\\n                   StructField(name = 'id',dataType=IntegerType()),\\\n                   StructField(name = 'name',dataType=StringType()),\\\n                   StructField(name = 'salary',dataType=IntegerType())])\n#these are used to define schema to DF and create complex columns like nested struct array and map columns\n#StructType is a collection of StructField\n\n\nstruct = spark.createDataFrame(row,schem)\nstruct.show()\nstruct.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4673a596-50a7-4c6a-a46a-c1c234d3e19b","inputWidgets":{},"title":"StructType & StructField"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|maheer|   300|\n|  2|  rafi|   200|\n+---+------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode,col,array\n\nrows = [(1,'maheer',[300,3000]),(2,'rafi',[200,2000])]\nschema = StructType([\\\n                    StructField('id',IntegerType()),\\\n                    StructField('name',StringType()),\\\n                    StructField('salary',ArrayType(IntegerType()))])\naray = spark.createDataFrame(rows,schema)\n\naray.show()\n\naray.printSchema()\n# we can use indexing as well in arrays to choose a specific index for output\naray.withColumn('bonus',aray.salary[0]).withColumn('salary_',aray.salary[1]).show()\n\n#explode, split , array , array_contains\nexp = spark.createDataFrame(rows,schema)\nexp.withColumn('EXPLODED',explode(exp.salary)).show()\nexp.show()\n#split\nrowss = [(1,'maheer','300,3000'),(2,'rafi','200,2000')]\nSplit = spark.createDataFrame(rowss,column)\n\nSplit=Split.withColumn('sal',split(col('salary'),','))\nSplit.show()\nSplitted =Split.withColumn('bonus',Split.sal[0]).withColumn('salary_',Split.sal[1])\nSplitted.show()\n#array\narray = Splitted.withColumn('combinedsalary',array(col('bonus'),col('salary_')))\narray.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b84d664a-df4d-4780-b886-936f48f1016e","inputWidgets":{},"title":"Arraytype"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3315173971172554>:7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m explode,col,array\n\u001B[1;32m      3\u001B[0m rows \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaheer\u001B[39m\u001B[38;5;124m'\u001B[39m,[\u001B[38;5;241m300\u001B[39m,\u001B[38;5;241m3000\u001B[39m]),(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrafi\u001B[39m\u001B[38;5;124m'\u001B[39m,[\u001B[38;5;241m200\u001B[39m,\u001B[38;5;241m2000\u001B[39m])]\n\u001B[1;32m      4\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\\\n\u001B[1;32m      5\u001B[0m                     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m,IntegerType()),\\\n\u001B[1;32m      6\u001B[0m                     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m,StringType()),\\\n\u001B[0;32m----> 7\u001B[0m                     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m,ArrayType(IntegerType()))])\n\u001B[1;32m      8\u001B[0m aray \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(rows,schema)\n\u001B[1;32m     10\u001B[0m aray\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'ArrayType' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'ArrayType' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-3315173971172554>:7\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m explode,col,array\n","\u001B[1;32m      3\u001B[0m rows \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaheer\u001B[39m\u001B[38;5;124m'\u001B[39m,[\u001B[38;5;241m300\u001B[39m,\u001B[38;5;241m3000\u001B[39m]),(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrafi\u001B[39m\u001B[38;5;124m'\u001B[39m,[\u001B[38;5;241m200\u001B[39m,\u001B[38;5;241m2000\u001B[39m])]\n","\u001B[1;32m      4\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\\\n","\u001B[1;32m      5\u001B[0m                     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m,IntegerType()),\\\n","\u001B[1;32m      6\u001B[0m                     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m,StringType()),\\\n","\u001B[0;32m----> 7\u001B[0m                     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m,ArrayType(IntegerType()))])\n","\u001B[1;32m      8\u001B[0m aray \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(rows,schema)\n","\u001B[1;32m     10\u001B[0m aray\u001B[38;5;241m.\u001B[39mshow()\n","\n","\u001B[0;31mNameError\u001B[0m: name 'ArrayType' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types  import MapType\nr = [('rafi',{'hair':'black','eye':'brown'}),('wafa',{'hair':'black','eye':'blue'})]\n# s = ['name','properties']\ns = StructType\nmaptype = spark.createDataFrame(r,s)\nmaptype.show()\n\nget = maptype.withColumn('eye',maptype.properties.getItem('eye')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6543b894-b162-4f4b-b393-38d92bb135a9","inputWidgets":{},"title":"Maptype"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nrow1 = Row(name='rafi',sal=2000)\n# print(row)\nrow2 = Row(name='rasu',sal=3000)\ndat =[row1,row2]\nroes = spark.createDataFrame(dat)\nroes.show()\n\n# the simplest way to crreate  a colum class is by lit()\nfrom pyspark.sql.functions import lit\n# col1 =lit('abcd')\n# print(type(col1))\ndt = [('name','genmder',22222)]\nsc = ['name','gender','sal']\ndff = spark.createDataFrame(dt,sc)\ndff1 = dff.withColumn('newcol',lit('newcolvalue'))\ndff1.show()\n\n\n\n# select\n\ndff.select(dff1.gender).show()\n# we can also use to mention the column name like col('gender') or dff['gender']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"da5a679f-9c5c-4df9-ad2a-471c64a71a89","inputWidgets":{},"title":"Row    &    Column   class"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import when\ndataa = [(1,'maheer','M',2000,'dev'),(2,'srjan','M',3000,'IT'),(3,'maheer','F',4000,'HR'),(3,'maheer','T',4000,'Admin'),(4,'kalyan','M',4000,'HR')]\nschemaa = ['id','name','gender','salary','dep']\nda=spark.createDataFrame(dataa,schemaa)\nda.show()\n\nda1 = da.select(da.id,\\\n                da.name,\\\n                when(da.gender=='M','male')\\\n                .when(da.gender=='F','female')\\\n                .otherwise('unknown')\\\n                .alias('gender'))\nda1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"93676cb0-a192-4479-ba14-61ce3e1d2e91","inputWidgets":{},"title":"When , otherwise"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# asc desc\nda.sort(da.salary.desc()).show()\ncl = [(1,'maheer','M','2000','dev'),(2,'srjan','M','3000','IT')]\nd1=spark.createDataFrame(cl,schemaa)\nd1.show()\n\n# cast\nd2 = d1.select(d1.id,d1.name,d1.salary.cast('int'))\nd2.printSchema()\n\n\n# like\nda.filter(da.name.like('_a%')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"829fae20-092b-4749-83cc-993f06320f84","inputWidgets":{},"title":"alias(), asc(), desc(), cast() & like() "}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["da.distinct().show()\n\nda.dropDuplicates(['name']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f4e3795c-c125-4a3d-9902-8bf71196231a","inputWidgets":{},"title":"distinct,  dropduplicates"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# da.sort(da.salary.asc()).show()\n\nda.orderBy(da.salary.desc()).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"15f7f8ae-a151-4303-a2ab-1dbfd036af7b","inputWidgets":{},"title":"orderby & sort()"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["dataa = [(1,'maheer','M',2000,'dev'),(2,'srjan','M',3000,'IT'),(3,'maheer','F',4000,'HR'),(3,'maheer','T',4000,'Admin'),(4,'kalyan','M',4000,'HR')]\nschemaa = ['id','name','gender','salary','dep']\n\ndataa2 = [(1,'mahe','M',2000,'dev'),(2,'sran','M',3000,'IT'),(3,'mahe','F',4000,'HR'),(3,'maheer','T',4000,'Admin'),(4,'kaan','M',4000,'HR')]\nschemaa2 = ['id','name','gender','salary','dep']\n\ndf11=spark.createDataFrame(dataa,schemaa)\ndf12=spark.createDataFrame(dataa2,schemaa2)\n\n# df11.show()\n# da1.show()\ndf11.union(df11).show()\nnewd=df11.unionAll(df11)\nnewd.distinct().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8a13f53f-ded2-407a-970c-3c4447720fec","inputWidgets":{},"title":"unioin &unionall"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["data = [(1,'rafi','M',500000,'IT'),\\\n       (2,'ref','M',700000,'HR'),\\\n       (3,'uddin','M',600000,'IT'),\\\n       (4,'md','M',900000,'IT')]\nschema = ['id','name','gender','salary','dep']\n\ndf= spark.createDataFrame(data,schema)\ndf.show()\ndf.groupBy('dep').count().show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3ddcbdf1-6540-4924-a1ce-1a78d2743edf","inputWidgets":{},"title":"groupBy"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+---+\n| id| name|gender|salary|dep|\n+---+-----+------+------+---+\n|  1| rafi|     M|500000| IT|\n|  2|  ref|     M|700000| HR|\n|  3|uddin|     M|600000| IT|\n|  4|   md|     M|900000| IT|\n+---+-----+------+------+---+\n\n+---+-----+\n|dep|count|\n+---+-----+\n| IT|    3|\n| HR|    1|\n+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["help(df.groupBy)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"076e3087-e79b-4710-b46e-110d161842e2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Help on method groupBy in module pyspark.sql.dataframe:\n\ngroupBy(*cols: 'ColumnOrName') -> 'GroupedData' method of pyspark.sql.dataframe.DataFrame instance\n    Groups the :class:`DataFrame` using the specified columns,\n    so we can run aggregation on them. See :class:`GroupedData`\n    for all the available aggregate functions.\n    \n    :func:`groupby` is an alias for :func:`groupBy`.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    cols : list, str or :class:`Column`\n        columns to group by.\n        Each element should be a column name (string) or an expression (:class:`Column`)\n        or list of them.\n    \n    Returns\n    -------\n    :class:`GroupedData`\n        Grouped data by given columns.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([\n    ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n    \n    Empty grouping columns triggers a global aggregation.\n    \n    >>> df.groupBy().avg().show()\n    +--------+\n    |avg(age)|\n    +--------+\n    |    2.75|\n    +--------+\n    \n    Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n    \n    >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n    +-----+--------+\n    | name|sum(age)|\n    +-----+--------+\n    |Alice|       2|\n    |  Bob|       9|\n    +-----+--------+\n    \n    Group-by 'name', and calculate maximum values.\n    \n    >>> df.groupBy(df.name).max().sort(\"name\").show()\n    +-----+--------+\n    | name|max(age)|\n    +-----+--------+\n    |Alice|       2|\n    |  Bob|       5|\n    +-----+--------+\n    \n    Group-by 'name' and 'age', and calculate the number of rows in each group.\n    \n    >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n    +-----+---+-----+\n    | name|age|count|\n    +-----+---+-----+\n    |Alice|  2|    1|\n    |  Bob|  2|    2|\n    |  Bob|  5|    1|\n    +-----+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["unionByName"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b766e8b2-7f89-4c52-87c0-6e978a2c3089","inputWidgets":{},"title":"unionByName"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.select(['id','name',]).show()\ndf.select('*').show"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b2e2a983-cb5a-401a-b093-0d3715f1f363","inputWidgets":{},"title":"select"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = [(1,'rafi','M',500000,'1'),\\\n       (2,'ref','M',700000,'2'),\\\n       (3,'uddin','M',600000,'1'),\\\n       (4,'md','M',900000,'1')]\nschema = ['id','name','gender','salary','dep']\ndata2 = [(1,'IT'),\\\n       (2,'HR'),(3,'payroll')]\nschema2 = ['dep_id','name']\nemp =spark.createDataFrame(data,schema)\ndep=spark.createDataFrame(data2,schema2)\n# emp.show()\n# dep.show()\n\n\n# emp.join(dep,emp.dep==dep.dep_id,'inner').show()\n# emp.join(dep,emp.dep==dep.dep_id,'right').show()\nemp.join(dep,emp.dep==dep.dep_id,'semi').show()\n\n \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ad353360-2600-4e32-a216-31ebffa1f177","inputWidgets":{},"title":"join"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+---+\n| id| name|gender|salary|dep|\n+---+-----+------+------+---+\n|  1| rafi|     M|500000|  1|\n|  3|uddin|     M|600000|  1|\n|  4|   md|     M|900000|  1|\n|  2|  ref|     M|700000|  2|\n+---+-----+------+------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"19d56987-a2b9-4727-889b-14598eb7bca6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n    .show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c91385ec-d710-402a-a149-ec5ada11a634","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n\ncolumns= [\"Product\",\"Amount\",\"Country\"]\ndf = spark.createDataFrame(data = data, schema = columns)\n\n\npivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\npivotDF.printSchema()\npivotDF.show(truncate=False)\n\n\nfrom pyspark.sql.functions import expr\nunpivotDF = pivotDF.select('Product',expr(\"stack(4,'Canada',canada,'China',china,'Mexico',mexico,'USA',Usa)as (Country,Total)\")).where(\"Total is not null\")\n\nunpivotDF.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"756d0477-16b3-43c3-b802-956202b79aa9","inputWidgets":{},"title":"pivot"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Product: string (nullable = true)\n |-- Canada: long (nullable = true)\n |-- China: long (nullable = true)\n |-- Mexico: long (nullable = true)\n |-- USA: long (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico|USA |\n+-------+------+-----+------+----+\n|Orange |null  |4000 |null  |4000|\n|Beans  |null  |1500 |2000  |1600|\n|Banana |2000  |400  |null  |1000|\n|Carrots|2000  |1200 |null  |1500|\n+-------+------+-----+------+----+\n\n+-------+-------+-----+\n|Product|Country|Total|\n+-------+-------+-----+\n|Orange |China  |4000 |\n|Orange |USA    |4000 |\n|Beans  |China  |1500 |\n|Beans  |Mexico |2000 |\n|Beans  |USA    |1600 |\n|Banana |Canada |2000 |\n|Banana |China  |400  |\n|Banana |USA    |1000 |\n|Carrots|Canada |2000 |\n|Carrots|China  |1200 |\n|Carrots|USA    |1500 |\n+-------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ea054de-5b92-426d-98ee-6993efab513a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"practice wafa","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
